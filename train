

# Importowanie bibliotek
from brain import Brain
from DQN import dqn
from environment import Environment
import numpy as np
import matplotlib.pyplot as plt
import pygame

pygame.init()
# Definiowanie parametrów
memSize = 60000
batchSize = 32
learningRate = 0.0001
gamma = 0.9
nLastStates = 4

epsilon = 1.
epsilonDecayRate = 0.0002
minEpsilon = 0.05

filepathToSave = 'model2.h5'
filepathToSave_prey='model3.h5'
# Tworzenie środowiska, mózgu i pamięci doświadczeń
env = Environment()
brain = Brain((env.screen_width, env.screen_height, nLastStates), learningRate)
model = brain.model
Dqn = dqn(memSize, gamma)
brain_prey = Brain((env.screen_width, env.screen_height, nLastStates), learningRate)
Dqn_prey = dqn(memSize,gamma)
model2 = brain_prey.model
def resetstates():
    currentstate = np.zeros((1, env.screen_width,env.screen_height, nLastStates))

    for i in range(nLastStates):
        currentstate[:, :, :, i] = env.screenMap

    return currentstate, currentstate


# Uruchomienie głównej pętli
epoch = 0
scores = list()
scores2 = list()
maxNCollected = 0
maxNCollected_prey = 0
nCollected = 0
nCollected_prey = 0
totNCollected = 0
totNCollected_prey = 0
while True:
    # Resetowanie środowiska i stanów gry
    env.reset()
    
    epoch += 1
    gameOver = False
    
    # Rozpoczęcie drugiej pętli, w której gramy i uczymy naszą sztuczną inteligencję
    while not gameOver:
        currentstate, nextstate = resetstates()
        # Wybór akcji do zagrania
        if np.random.rand() < epsilon:
            action = np.random.randint(0, 3)
        else:
            qvalues = model.predict(currentstate)[0]
            action = np.argmax(qvalues)
        if np.random.rand() < epsilon:
            action_prey = np.random.randint(0, 3)
        else:
            qvalues2 = model2.predict(currentstate)[0]
            action_prey = np.argmax(qvalues2)
        # Aktualizacja środowiska
        gameOver, reward, collected,state = env.update_predator_food(action)
        reward_prey,collected_prey = env.update_prey(action_prey)
        print(gameOver)
        nCollected += collected

        nCollected_prey += collected_prey

        # Dodanie nowej klatki gry do następnego stanu i usunięcie najstarszej klatki z następnego stanu
        state = np.reshape(state, (1, env.screen_width, env.screen_height, 1))
        nextState = np.append(nextstate, state, axis = 3)
        nextState = np.delete(nextState, 0, axis = 3)
        
        # Zapamiętywanie przejścia i szkolenie naszej sztucznej inteligencji
        Dqn.remember([currentstate, action, reward, nextState], gameOver)
        Dqn_prey.remember([currentstate, action_prey, reward_prey, nextState], gameOver)
        inputs, targets = Dqn.get_batch(model, batchSize)
        model.train_on_batch(inputs, targets)
        inputs_prey,targets_prey = Dqn_prey.get_batch(model2,batchSize)
        model2.train_on_batch(inputs_prey, targets_prey)
        
        currentstate = nextState

    if nCollected > maxNCollected and nCollected > 2:
        maxNCollected = nCollected
        model.save(filepathToSave)
    if nCollected_prey > maxNCollected_prey and nCollected > 15:
        maxNCollected_prey = nCollected
        model2.save(filepathToSave_prey)
    totNCollected += nCollected
    nCollected = 0
    totNCollected_prey += nCollected_prey
    nCollected_prey = 0
    # Wyświetlanie wyników co 100 gier
    if epoch % 100 == 0 and epoch != 0:
        scores.append(totNCollected / 100)
        scores2.append(totNCollected_prey/100)
        totNCollected = 0
        totNCollected_prey = 0
        plt.plot(scores)
        plt.xlabel('Epoch / 100')
        plt.ylabel('Average Score')
        plt.savefig('stats.png')
        plt.close()
    
    # Zmniejszenie wartości epsilon
    if epsilon > minEpsilon:
        epsilon -= epsilonDecayRate
    
    # Wyświetlanie wyników każdej gry
    print('Epoch: ' + str(epoch) + ' Current Best predator: ' + str(maxNCollected) + 'prey:' + str(maxNCollected))
    print('Epsilon: {:.5f}'.format(epsilon))
